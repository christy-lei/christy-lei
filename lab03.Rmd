---
title: "Lab 3"
author: "Christy Lei"
date: "Math 241, Week 4"
output:
  pdf_document
urlcolor: blue
---

```{r setup, include=FALSE}
# Do not modify this chunk.
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

```{r}
# Put all necessary libraries here
library(tidyverse)
library(ggplot2)
library(ggthemes)
```


## Due: Thursday, February 27th at 8:30am

## Goals of this lab

1. Practice using GitHub.
1. Practice wrangling data.


## Data Notes:

* For Problem 2, we will continue to dig into the SE Portland crash data but will use two datasets:
    + `CRASH`: crash level data
    + `PARTIC`: participant level data

```{r}
# Crash level dataset
crash <- read_csv("/home/courses/math241s20/Data/pdx_crash_2018_CRASH.csv")

# Participant level dataset
partic <- read_csv("/home/courses/math241s20/Data/pdx_crash_2018_PARTIC.csv")
```

* For Problem 3, we will look at chronic illness data from the [CDC](https://www.cdc.gov/cdi/index.html) along with the regional mapping for each state.

```{r}
# CDC data
CDC <- read_csv("/home/courses/math241s20/Data/CDC2.csv")

# Regional data
USregions <- read_csv("/home/courses/math241s20/Data/USregions.csv")
```

* For Problem 4, we will use polling data from [FiveThirtyEight.com](https://projects.fivethirtyeight.com/congress-generic-ballot-polls/).


```{r}
# Note I only want us to focus on a subset of the variables
polls <- read_csv("/home/courses/math241s20/Data/generic_topline.csv") %>%
  select(subgroup, modeldate, dem_estimate, rep_estimate)
```


## Problems


### Problem 1: Git Control

In this problem, we will practice interacting with GitHub on the site directly and from the RStudio Server.  Do this practice on **your repo**, not your group's Project 1 repo, so that the graders can check your progress with Git.

a. Let's practice creating and closing **Issues**.  In a nutshell, **Issues** let us keep track of our work. Within your repo on GitHub.com, create an Issue entitled "Complete Lab 3".  Once Lab 3 is done, close the **Issue**.  (If you want to learn more about the functionalities of Issues, check out this [page](https://guides.github.com/features/issues/).)


b. Edit the ReadMe of your repo to include your name and a quick summary of the purpose of the repo.  You can edit from within GitHub directly or on the server.  If you edit on the server, make sure to push your changes to GitHub.

c. Upload both your Lab 3 .Rmd and .pdf to your repo on GitHub.


### Problem 2: `dplyr` madness

Each part of this problem will require you to wrangle the data and then do one or both of the following:

+ Display the wrangled data frame.  To ensure it displays the whole data frame, you can pipe `as.data.frame()` at the end of the wrangling.
+ Answer a question(s).

**Some parts will require you to do a data join but won't tell you that.**



a. Produce a table that provides the frequency of the different collision types, ordered from most to least common.  What type is most common? What type is least common?

```{r}
crash %>% 
  count(COLLIS_TYP_CD) %>% 
  arrange(desc(n)) %>% 
  as.data.frame()
```
Rear-End (type 3) is the most common collision type and Miscellaneous (type &) is the least common one.


b.  For the three most common collision types, create a table that contains:
    + The frequencies of each collision type and weather condition combination.
    + The proportion of each collision type by weather condition.
    
Arrange the table by weather and within type, most to least common collision type.  
```{r}
crash %>% 
  count(COLLIS_TYP_CD, WTHR_COND_CD) %>%
  group_by(WTHR_COND_CD) %>% 
  mutate(prop = prop.table(n)) %>%
  arrange(desc(n)) %>% 
  as.data.frame()
```


c. Create a column for whether or not a crash happened on a weekday or on the weekend and then create a data frame that explores if the distribution of collision types varies by whether or not the crash happened during the week or the weekend.

```{r}
weekday = c(1,2,3,4,5)
weekend = c(6,7)
crash$weekday_true = 
  ifelse(crash$CRASH_WK_DAY_CD %in% weekday, "Yes",
  ifelse(crash$CRASH_WK_DAY_CD %in% weekend, "No", NA))

crash_weekday <- crash %>% 
  count(COLLIS_TYP_CD, weekday_true) %>%
  group_by(weekday_true) %>%
  mutate(prop = prop.table(n)) %>%
  arrange(desc(n))

crash_weekday

```
It seems that the most common collision type (Type 3) happened mostly on weekdays. 


d.  First determine what proportion of crashes involve pedestrians.  Then, for each driver license status, determine what proportion of crashes involve pedestrians.  What driver license status has the highest rate of crashes that involve pedestrians?

```{r}
#proportion of crashes that involve pedestrians
crash %>% 
  count(pedestrians_involved = (CRASH_TYP_CD == 3)) %>%
  mutate(prop = prop.table(n))

#proportion of crashes that involve pedestrians for each driver license status
crash_and_partic <- left_join (crash, partic, by = c("CRASH_ID" = "CRASH_ID")) 
crash_and_partic %>%
  group_by(DRVR_LIC_STAT_CD) %>%
  count(pedestrians_involved = (CRASH_TYP_CD == 3)) %>%
  mutate(prop = prop.table(n)) %>%
  arrange(desc(n))

```
5.49% of all the crashs involve pedestrians (code 3).\n Driver license type 1 (Valid Oregon license or permit) has the highest rate of crashs (2.9%) that involve pedestrians.


e. Create a data frame that contains the age of drivers and collision type. (Don't print it.)  Complete the following:
    + Find the average and median age of drivers.
    + Find the average and median age of drivers by collision type.
    + Create a graph of driver ages.
    + Create a graph of driver ages by collision type.
    
Draw some conclusions.

```{r}
collision_type_age <- crash_and_partic %>% 
  select(COLLIS_TYP_CD, AGE_VAL) 

# Find the average and median age of drivers
collision_type_age %>%
  summarize(mean_age = mean(as.numeric(AGE_VAL)), 
            median_age = median(as.numeric(AGE_VAL)))

# Find the average and median age of drivers by collision type
collision_type_age %>%
  group_by(COLLIS_TYP_CD) %>%
  summarize(mean_age = mean(as.numeric(AGE_VAL)), 
            median_age = median(as.numeric(AGE_VAL)))

# Create a graph of driver ages
collision_type_age %>% 
  ggplot(aes(x = AGE_VAL)) +
  geom_bar()

# Create a graph of driver ages by collision type
collision_type_age %>% 
  ggplot(aes(x = AGE_VAL, fill = COLLIS_TYP_CD)) +
  geom_dotplot(binwidth = 0.8) +
  scale_fill_brewer(palette = "Accent") +
  theme_fivethirtyeight() +
  theme(panel.grid = element_blank(),
        plot.title = element_text(size = 16)) +
   ylim(0,900) + 
  labs(title = "Driver ages by collision type")
```



### Problem 3: Chronically Messy Data

a. Turning to the CDC data, let's get a handle of what is represented there.  For 2016 (use `YearStart`), how many distinct topics were tracked?

```{r}
CDC %>% 
  filter(YearStart == 2016) %>%
  count(Topic) %>%
  nrow()
```
16 distinct topics were tracked. 


b. Let's study influenza vaccination patterns! Create a dataset that contains the age adjusted prevalence of the "Influenza vaccination among noninstitutionalized adults aged >= 18 years" for Oregon and the US from 2010 to 2016.  

```{r}
influenza_vacc_prevalance <- CDC %>%
  filter(YearStart %in% c(2010, 2011, 2012, 2013, 2014, 2015, 2016),
         Question == "Influenza vaccination among noninstitutionalized adults aged >= 18 years",
         DataValueType == "Age-adjusted Prevalence") %>%
  select(YearStart, LocationAbbr, DataValue, DataValueType, Question) %>%
  arrange(YearStart) %>% 
  pivot_wider(names_from = DataValueType,
              values_from = DataValue) %>%
  as.data.frame()

influenza_vacc_oregon_us <- influenza_vacc_prevalance %>%
  filter(LocationAbbr%in% c("OR", "US")) 

influenza_vacc_oregon_us

```

c. Create a graph comparing the immunization rates of Pennsylvania and the US.  Comment on the observed trends in your graph
```{r}
influenza_vacc_prevalance %>% 
  filter(LocationAbbr %in% c("PA", "US")) %>%
  ggplot(aes(x = YearStart, y = `Age-adjusted Prevalence`, color = LocationAbbr)) +
  geom_line(size = 1)

```


 
d.  Let's see how immunization rates vary by region of the country. Join the regional dataset to our CDC dataset so that we have a column signifying the region of the country.  
```{r}
CDC <- left_join (CDC, USregions, by = c("LocationDesc" = "State")) 
```


e. Why are there NAs in the region column of the new dataset?

left_join; United States.

f. Create a dataset that contains the age adjusted influenza immunization rates in 2016 for each state in the country and sort it by highest immunization to lowest.  Which state has the highest immunization? 





g. Construct a graphic of the 2016 influenza immunization rates by region of the country.  Don't include locations without a region. Comment on your graphic.







### Problem 4: Tidying Data Like a Boss

I was amazed by the fact that many of the FiveThirtyEight datasets are actually not in a perfectly *tidy* format.  Let's tidy up this dataset related to [polling](https://projects.fivethirtyeight.com/congress-generic-ballot-polls/).  



a. Why is this data not currently in a tidy format?

Each row contains mutiple observations.

```{r}
polls
```


b. Create a tidy dataset of the `All polls` subgroup.

```{r}
polls %>% pivot_wider(names_from = subgroup,
              values_from = c(dem_estimate, rep_estimate))
```

c. Now let's create a new untidy version of `polls`.  Focusing just on the estimates for democrats, create a data frame where each row represents a subgroup (given in column 1) and the rest of the columns are the estimates for democrats by date.
```{r}
polls %>% 
  select(!rep_estimate) %>%
  pivot_wider(names_from = modeldate,
              values_from = dem_estimate)
```


d. Why might someone want to transform the data like we did in part c? 
+ look at one date only and compare the three
+ see the trend 


### Problem 5: YOUR TURN!

Now it is your turn.  Pick one (or multiple) of the datasets used on this lab.  Ask a question of the data.  Do some data wrangling to produce statistics (use at least two wrangling verbs) and a graphic to answer the question.  Then comment on any conclusions you can draw about your question.

