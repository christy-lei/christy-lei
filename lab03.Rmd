---
title: "Lab 3"
author: "Christy Lei"
date: "Math 241, Week 4"
output:
  pdf_document
urlcolor: blue
---

```{r setup, include=FALSE}
# Do not modify this chunk.
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

```{r}
# Put all necessary libraries here
library(tidyverse)
library(ggplot2)
library(ggthemes)
```


## Due: Thursday, February 27th at 8:30am

## Goals of this lab

1. Practice using GitHub.
1. Practice wrangling data.


## Data Notes:

* For Problem 2, we will continue to dig into the SE Portland crash data but will use two datasets:
    + `CRASH`: crash level data
    + `PARTIC`: participant level data

```{r}
# Crash level dataset
crash <- read_csv("/home/courses/math241s20/Data/pdx_crash_2018_CRASH.csv")

# Participant level dataset
partic <- read_csv("/home/courses/math241s20/Data/pdx_crash_2018_PARTIC.csv")
```

* For Problem 3, we will look at chronic illness data from the [CDC](https://www.cdc.gov/cdi/index.html) along with the regional mapping for each state.

```{r}
# CDC data
CDC <- read_csv("/home/courses/math241s20/Data/CDC2.csv")

# Regional data
USregions <- read_csv("/home/courses/math241s20/Data/USregions.csv")
```

* For Problem 4, we will use polling data from [FiveThirtyEight.com](https://projects.fivethirtyeight.com/congress-generic-ballot-polls/).


```{r}
# Note I only want us to focus on a subset of the variables
polls <- read_csv("/home/courses/math241s20/Data/generic_topline.csv") %>%
  select(subgroup, modeldate, dem_estimate, rep_estimate)
```


## Problems


### Problem 1: Git Control

In this problem, we will practice interacting with GitHub on the site directly and from the RStudio Server.  Do this practice on **your repo**, not your group's Project 1 repo, so that the graders can check your progress with Git.

a. Let's practice creating and closing **Issues**.  In a nutshell, **Issues** let us keep track of our work. Within your repo on GitHub.com, create an Issue entitled "Complete Lab 3".  Once Lab 3 is done, close the **Issue**.  (If you want to learn more about the functionalities of Issues, check out this [page](https://guides.github.com/features/issues/).)


b. Edit the ReadMe of your repo to include your name and a quick summary of the purpose of the repo.  You can edit from within GitHub directly or on the server.  If you edit on the server, make sure to push your changes to GitHub.

c. Upload both your Lab 3 .Rmd and .pdf to your repo on GitHub.


### Problem 2: `dplyr` madness

Each part of this problem will require you to wrangle the data and then do one or both of the following:

+ Display the wrangled data frame.  To ensure it displays the whole data frame, you can pipe `as.data.frame()` at the end of the wrangling.
+ Answer a question(s).

**Some parts will require you to do a data join but won't tell you that.**



a. Produce a table that provides the frequency of the different collision types, ordered from most to least common.  What type is most common? What type is least common?

```{r}
crash %>% 
  count(COLLIS_TYP_CD) %>% 
  arrange(desc(n)) %>% 
  as.data.frame()
```
Rear-End (type 3) is the most common collision type and Miscellaneous (type &) is the least common one.


b.  For the three most common collision types, create a table that contains:
    + The frequencies of each collision type and weather condition combination.
    + The proportion of each collision type by weather condition.
    
Arrange the table by weather and within type, most to least common collision type.  
```{r}
crash %>% 
  count(COLLIS_TYP_CD, WTHR_COND_CD) %>%
  group_by(WTHR_COND_CD) %>% 
  mutate(prop = prop.table(n)) %>%
  arrange(desc(n)) %>% 
  as.data.frame()
```


c. Create a column for whether or not a crash happened on a weekday or on the weekend and then create a data frame that explores if the distribution of collision types varies by whether or not the crash happened during the week or the weekend.

```{r}
weekday = c(1,2,3,4,5)
weekend = c(6,7)
crash$weekday_true = 
  ifelse(crash$CRASH_WK_DAY_CD %in% weekday, "Yes",
  ifelse(crash$CRASH_WK_DAY_CD %in% weekend, "No", NA))

crash_weekday <- crash %>% 
  count(COLLIS_TYP_CD, weekday_true) %>%
  group_by(weekday_true) %>%
  mutate(prop = prop.table(n)) %>%
  arrange(desc(n))

crash_weekday

```
It seems that the most common collision type (Type 3) happened mostly on weekdays. 


d.  First determine what proportion of crashes involve pedestrians.  Then, for each driver license status, determine what proportion of crashes involve pedestrians.  What driver license status has the highest rate of crashes that involve pedestrians?

```{r}
#proportion of crashes that involve pedestrians
crash %>% 
  count(pedestrians_involved = (CRASH_TYP_SHORT_DESC == "PED")) %>%
  mutate(prop = prop.table(n))

#proportion of crashes that involve pedestrians for each driver license status
crash_and_partic <- left_join (crash, partic, by = c("CRASH_ID" = "CRASH_ID")) 
crash_and_partic %>%
  group_by(DRVR_LIC_STAT_CD) %>%
  count(pedestrians_involved = (CRASH_TYP_SHORT_DESC == "PED")) %>%
  mutate(prop = prop.table(n)) %>%
  filter(pedestrians_involved == TRUE) %>%
  arrange(desc(n))

```
5.49% of all the crashs involve pedestrians. Driver license type 1 (Valid Oregon license or permit) has the highest rate of crashs (2.9%) that involve pedestrians.


e. Create a data frame that contains the age of drivers and collision type. (Don't print it.)  Complete the following:
    + Find the average and median age of drivers.
    + Find the average and median age of drivers by collision type.
    + Create a graph of driver ages.
    + Create a graph of driver ages by collision type.
    
Draw some conclusions.

```{r }
collision_type_age <- crash_and_partic %>% 
  select(COLLIS_TYP_CD, AGE_VAL) 

# Find the average and median age of drivers
collision_type_age %>%
  summarize(mean_age = mean(as.numeric(AGE_VAL)), 
            median_age = median(as.numeric(AGE_VAL)))

# Find the average and median age of drivers by collision type
collision_type_age %>%
  group_by(COLLIS_TYP_CD) %>%
  summarize(mean_age = mean(as.numeric(AGE_VAL)), 
            median_age = median(as.numeric(AGE_VAL)))

# Create a graph of driver ages
collision_type_age %>% 
  ggplot(aes(x = AGE_VAL)) +
  geom_bar()

# Create a graph of driver ages by collision type
collision_type_age %>% 
  filter(AGE_VAL != "00") %>% #filtering out the unknown age
  ggplot(aes(x = as.numeric(AGE_VAL), fill = COLLIS_TYP_CD)) +
  geom_histogram(binwidth = 2) +
  scale_fill_brewer(palette = 3, direction = -1) +
  scale_x_continuous(breaks = c(0,10,20,30,40,50,60,70,80,90)) +
  theme(plot.title = element_text(size = 16)) +
  theme(axis.text.x = element_text(size = 10)) +
  labs(title = "Driver ages by collision type",
       x = "Age", y = "Number of Collisions", fill = "Collision Type")
```



### Problem 3: Chronically Messy Data

a. Turning to the CDC data, let's get a handle of what is represented there.  For 2016 (use `YearStart`), how many distinct topics were tracked?

```{r}
CDC %>% 
  filter(YearStart == 2016) %>%
  count(Topic) %>%
  nrow()
```
16 distinct topics were tracked. 


b. Let's study influenza vaccination patterns! Create a dataset that contains the age adjusted prevalence of the "Influenza vaccination among noninstitutionalized adults aged >= 18 years" for Oregon and the US from 2010 to 2016.  

```{r}
influenza_vacc_prevalance <- CDC %>%
  filter(YearStart %in% c(2010, 2011, 2012, 2013, 2014, 2015, 2016),
         Question == "Influenza vaccination among noninstitutionalized adults aged >= 18 years",
         DataValueType == "Age-adjusted Prevalence") %>%
  select(YearStart, LocationAbbr, DataValue, DataValueType, Question) %>%
  arrange(YearStart) %>% 
  pivot_wider(names_from = DataValueType,
              values_from = DataValue) %>%
  as.data.frame()

influenza_vacc_oregon_us <- influenza_vacc_prevalance %>%
  filter(LocationAbbr%in% c("OR", "US")) 

influenza_vacc_oregon_us

```

c. Create a graph comparing the immunization rates of Pennsylvania and the US.  Comment on the observed trends in your graph
```{r}
influenza_vacc_prevalance %>% 
  filter(LocationAbbr %in% c("PA", "US")) %>%
  ggplot(aes(x = YearStart, y = `Age-adjusted Prevalence`, 
             color = LocationAbbr)) +
  geom_line(size = 1)

```
  For both Pennsylvania and the US, there is a steady increase in immunization rates from 2011 to 2015 (with the exception of a slight drop in 2012) and there was a huge decrease in immunization rates in 2016.
 
 In general, the immunization rate in Pennslyvania is lower than that in the US with the exception of year 2015 and 2016.
 
d.  Let's see how immunization rates vary by region of the country. Join the regional dataset to our CDC dataset so that we have a column signifying the region of the country.  
```{r}
CDC <- left_join (CDC, USregions, by = c("LocationDesc" = "State")) 
```


e. Why are there NAs in the region column of the new dataset?

A left join in question (d) returns all rows from CDC, and all columns from CDC and USregions. However, some location names used in the CDC dataset (55 locations for the LocationDesc variable) are not present in the dataset USregions (50 locations for the State variable), so this creates NAs for the variable Region in the new dataset.


f. Create a dataset that contains the age adjusted influenza immunization rates in 2016 for each state in the country and sort it by highest immunization to lowest.  Which state has the highest immunization? 

```{r}
immunization_by_state <- CDC %>%
  filter(YearStart == 2016, 
         Question == "Influenza vaccination among noninstitutionalized adults aged >= 18 years",
         DataValueType == "Age-adjusted Prevalence") %>% 
  select(YearStart, LocationAbbr, DataValue, DataValueType, Question, Region) %>%
  pivot_wider(names_from = DataValueType,
              values_from = DataValue) %>%
  arrange(desc(`Age-adjusted Prevalence`)) %>%
  as.data.frame() 

immunization_by_state 

```

South Dakota has the highest immunization rate.


g. Construct a graphic of the 2016 influenza immunization rates by region of the country.  Don't include locations without a region. Comment on your graphic.
```{r}
immunization_by_state %>%
  drop_na() %>%
  ggplot(aes(x = Region, y = `Age-adjusted Prevalence`, fill = Region)) +
  geom_boxplot() + geom_point(width = 1, alpha = 0.2) +
  labs(y = "Age-adjusted Prevalence Rate") 
```

In 2016, NE has the highest median of the influenza immunization rates compared to other regions, and MW has the largest IQR which means the immunization rates are the most spread. FOR S and W, the influenza immunization rates are similar in terms of the average (i.e. median) and variability (i.e. IQR).


### Problem 4: Tidying Data Like a Boss

I was amazed by the fact that many of the FiveThirtyEight datasets are actually not in a perfectly *tidy* format.  Let's tidy up this dataset related to [polling](https://projects.fivethirtyeight.com/congress-generic-ballot-polls/).  



a. Why is this data not currently in a tidy format?

Some rows contain mutiple observations.

```{r}
polls
```


b. Create a tidy dataset of the `All polls` subgroup.

```{r}
polls %>% pivot_wider(names_from = subgroup,
              values_from = c(dem_estimate, rep_estimate))
```

c. Now let's create a new untidy version of `polls`.  Focusing just on the estimates for democrats, create a data frame where each row represents a subgroup (given in column 1) and the rest of the columns are the estimates for democrats by date.
```{r}
untidy_poll <- polls %>% 
  select(!rep_estimate) %>%
  pivot_wider(names_from = modeldate,
              values_from = dem_estimate) 

untidy_poll %>%
  ggplot(aes())
```


d. Why might someone want to transform the data like we did in part c? 
+ They are interested in comparing estimates of votes from the three subgroups on a particular day 
+ They just want to scroll horizontally to see if there's a general trend of increasing or decreasing number of estimates. 


### Problem 5: YOUR TURN!

Now it is your turn.  Pick one (or multiple) of the datasets used on this lab.  Ask a question of the data.  Do some data wrangling to produce statistics (use at least two wrangling verbs) and a graphic to answer the question.  Then comment on any conclusions you can draw about your question.

+ For the CDC dataset: I want to explore more for the question "Recent mentally unhealthy days among adults aged >= 18 years". I'm curious to learn about the trend of the mean number of mentally unhealthy days among adults from year 2010 to 2016, and how that differs in each region.

```{r fig.width = 8}
mental_health_region <- CDC %>%
  filter(YearStart %in% c(2010,2011,2012,2013,2014,2015,2016), 
         Question == "Recent mentally unhealthy days among adults aged >= 18 years",
         DataValueType == "Mean") %>% 
  select(YearStart, DataValue, DataValueType, Question, Region, LocationAbbr) %>%
  pivot_wider(names_from = DataValueType,
              values_from = DataValue) %>%
  arrange(desc(YearStart)) %>%
  as.data.frame() 

mental_health_region %>% 
  drop_na() %>%
  ggplot(aes(x = factor(YearStart), y = Mean, fill = Region)) +
  geom_boxplot() +
  geom_jitter(width = 0.1, alpha = 0.2) +
  xlab("Year") +
  ylab("Mean of recent mentally unhealthy days among adults") +
  facet_wrap(~Region, ncol = 4) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

In S, adults consistanly experience more mentally unhealthy days compared to other regions. For all regions, it seems that there the mean number of mentally unstable days stayed the same throughout the years. 

